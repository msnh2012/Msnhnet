config:
  batch: 1
  width: 224
  height: 224
  channels: 3

#0-1
conv:
  batchNorm: 0
  filters: 64
  kSize: 7
  stride: 2
  padding: 3
  useBias: 0
  activation: none
batchnorm:
  activation: relu

maxpool:
  kSize: 3
  stride: 2
  padding: 1

# Layer1
addblock:
  size: 1
  branch:
    conv:
      batchNorm: 0
      filters: 64
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 64
      kSize: 3
      stride: 1
      padding: 1
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 256
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  branch:
    conv:
      batchNorm: 0
      filters: 256
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  activation: relu

addblock:
  size: 2
  branch:
    conv:
      batchNorm: 0
      filters: 64
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 64
      kSize: 3
      stride: 1
      padding: 1
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 256
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none
  
  branch:
    empty:

  activation: relu

# Layer2
addblock:
  size: 1
  branch:
    conv:
      batchNorm: 0
      filters: 128
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 128
      kSize: 3
      stride: 2
      padding: 1
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 512
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  branch:
    conv:
      batchNorm: 0
      filters: 512
      kSize: 1
      stride: 2
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  activation: relu

addblock:
  size: 7
  branch:
    conv:
      batchNorm: 0
      filters: 128
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 128
      kSize: 3
      stride: 1
      padding: 1
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 512
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  branch:
    empty:
    
  activation: relu

# Layer3
addblock:
  size: 1
  branch:
    conv:
      batchNorm: 0
      filters: 256
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 256
      kSize: 3
      stride: 2
      padding: 1
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 1024
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  branch:
    conv:
      batchNorm: 0
      filters: 1024
      kSize: 1
      stride: 2
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none
  
  activation: relu

addblock:
  size: 35
  branch:
    conv:
      batchNorm: 0
      filters: 256
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 256
      kSize: 3
      stride: 1
      padding: 1
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 1024
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  branch:
    empty:
    
  activation: relu

# Layer4
addblock:
  size: 1
  branch:
    conv:
      batchNorm: 0
      filters: 512
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 512
      kSize: 3
      stride: 2
      padding: 1
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 2048
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  branch:
    conv:
      batchNorm: 0
      filters: 2048
      kSize: 1
      stride: 2
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none
  
  activation: relu

addblock:
  size: 2
  branch:
    conv:
      batchNorm: 0
      filters: 512
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 512
      kSize: 3
      stride: 1
      padding: 1
      useBias: 0
      activation: none

    batchnorm:
      activation: relu

    conv:
      batchNorm: 0
      filters: 2048
      kSize: 1
      stride: 1
      padding: 0
      useBias: 0
      activation: none

    batchnorm:
      activation: none

  branch:
    empty:
    
  activation: relu

# output

localavgpool:
  kSize: 7
  stride: 1
  padding: 0

connect:
  output: 1000
  activation: none
